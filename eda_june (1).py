# -*- coding: utf-8 -*-
"""EDA_june.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G22gsfXX7eTV5TQDMyaKXhSVQQOucNiC
"""

# Import the libraries
import numpy as np # mathematical or dealing with nd arrays
import matplotlib.pyplot as plt # visualization
import pandas as pd # data manipulation, exploration and cleaning
import seaborn as sns # visualization
import warnings # to avoid getting warnings
warnings.filterwarnings('ignore')

# import the dataset
df= pd.read_csv('online.csv')

# to check the top 5 rows
df.head(3)

# last 5 rows
df.tail(3)

# to check the null values
df.isnull().sum()

df.shape

# dropping the columns
df.drop(columns=["Unnamed: 0","Unnamed: 13","9","#@%"],inplace=True)

df.columns

df.info()

df.isnull().sum()

df1=df.copy()

df["Age"].plot(kind="box")

df["Age"].median()

df["Age"].fillna(df["Age"].median(),inplace=True)

df.isnull().sum()

df["Family size"].plot(kind="box")

int(df["Family size"].mean())

df["Family size"].fillna(int(df["Family size"].mean()),inplace=True)

df.isnull().sum()

df.describe()["Age"]

q1=df.describe()["Age"]["25%"]
q3=df.describe()["Age"]["75%"]

IQR=q3-q1

upper_limit=q3+1.5*IQR
lower_limit=q1-1.5*IQR

upper_limit

lower_limit

new_df=df[(df["Age"]<upper_limit) & (df["Age"]>lower_limit)]

388-364



outliers=df[(df["Age"]>upper_limit) | (df["Age"]<lower_limit)]

df.shape

df['Age'].clip(lower_limit,upper_limit,inplace=True)

df["Age"].plot(kind="box")

df.head()

df["Monthly Income"].value_counts()

# Nominal columns-Marital_status,Gender,employment_status,
# Ordinal columns- Monthly income,Reviews, Educational Qualifications

df["Marital Status"].value_counts()

df=pd.get_dummies(df,columns=["Marital Status","Gender","employment_status"],dtype=int,drop_first=True)

df.head()

df["Educational Qualifications"].value_counts()

d={"Uneducated":1,"School":2,"Graduate":3,"Post Graduate":4,"Ph.D":5}

df["Educational Qualifications"]=df["Educational Qualifications"].map(d)

df.isnull().sum()

df.head()

df["Monthly Income"]=df["Monthly Income"].str.strip()
df["Reviews"]=df["Reviews"].str.strip()

df.isnull().sum()

df["Reviews"].value_counts()

df["Reviews"]=df["Reviews"].map({"Positive":2,"Negative":1})

df.head()

df["Monthly Income"].value_counts()

d1={"No Income":1,"Below Rs.10000":2,"10001 to 25000":3,"25001 to 50000":4,"More than 50000":5}

df["Monthly Income"]=df["Monthly Income"].map(d1)

df.head()

df["order_status"].value_counts()

d2={"Yes":1,"No":0}

df["order_status"]=df["order_status"].map(d2)

df.head()

df_norm=df.copy()
df_stand=df.copy()

df_stand.head()

"""Applying Standardization"""

from sklearn.preprocessing import StandardScaler

ss=StandardScaler()

data=ss.fit_transform(df_stand)

df_stand.columns

new_df_standard_scaled=pd.DataFrame(data,columns=df_stand.columns)

# apply normalization

df_norm.head()

from sklearn.preprocessing import MinMaxScaler

mm=MinMaxScaler()

data=mm.fit_transform(df_norm)

new_df_normalized=pd.DataFrame(data,columns=df_norm.columns)











sns.get_dataset_names()











# Import the necessary libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

meanmedian=pd.read_csv("/content/meanmedian.csv")

meanmedian.head()

meanmedian.isnull().sum()

meanmedian["Fare"].plot(kind="box")

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# pip install gradio

# median imputation technique
from sklearn.impute import SimpleImputer

si=SimpleImputer(strategy="median")

meanmedian["Fare"]=si.fit_transform(meanmedian[["Fare"]])

meanmedian.head()

sis=SimpleImputer(strategy="mean")

meanmedian["Age"]=sis.fit_transform(meanmedian[["Age"]])

meanmedian.isnull().sum()

class simpe:
  def func(self):
    return "hi there"
  def func2(self):
    return "hello there"

s=simpe()
s.func()

mode=pd.read_csv("/content/mode.csv")

mode.head()

mode.isnull().sum()

meanmedian["Age"].mean()

mode["FireplaceQu"].mode()[0]

mode["FireplaceQu"].value_counts()

mode["FireplaceQu"].fillna(mode["FireplaceQu"].mode()[0],inplace=True)

mode.isnull().sum()

s3=SimpleImputer(strategy="most_frequent")

mode[["GarageQual"]]=s3.fit_transform(mode[["GarageQual"]])

mode.isnull().sum()

taxis=sns.load_dataset("taxis")

taxis.head()

taxis["pickup_year"]=taxis["pickup"].dt.year

taxis.columns

taxis["pickup_month"]=taxis["pickup"].dt.month

taxis["pickup_day"]=taxis["pickup"].dt.day

taxis.head()

df=pd.read_csv("/content/stud (1).csv")

df.head()

# Univariate Analysis

cat=df.select_dtypes(include="object")

num=df.select_dtypes(exclude="object")

num.columns

cat.columns

plt.figure(figsize=(30,6))
plt.subplot(1,3,1)
sns.histplot(df["math_score"],color="g")
plt.subplot(1,3,2)
sns.histplot(df["reading_score"],color="r")
plt.subplot(1,3,3)
sns.histplot(df["writing_score"],color="y")

plt.figure(figsize=(30,6))
plt.subplot(1,3,1)
sns.boxplot(df["math_score"],color="g")
plt.subplot(1,3,2)
sns.boxplot(df["reading_score"],color="r")
plt.subplot(1,3,3)
sns.boxplot(df["writing_score"],color="y")

cat.columns

plt.figure(figsize=(30,6))
plt.subplot(1,5,1)
df["gender"].value_counts().plot(kind="pie",autopct="%0.1f%%")
# let's plot the pie charts for rest of the cat columns
plt.subplot(1,5,2)
df["race_ethnicity"].value_counts().plot(kind="pie",autopct="%0.1f%%")
plt.subplot(1,5,3)
df["parental_level_of_education"].value_counts().plot(kind="pie",autopct="%0.1f%%")
plt.subplot(1,5,4)
df["lunch"].value_counts().plot(kind="pie",autopct="%0.1f%%")
plt.subplot(1,5,5)
df["test_preparation_course"].value_counts().plot(kind="pie",autopct="%0.1f%%")



sns.scatterplot(x=df["math_score"],y=df["reading_score"],hue=df["lunch"])

# test_preparation_course has any effect on each of the scores
plt.figure(figsize=(30,6))
plt.subplot(1,3,1)
sns.boxplot(x="test_preparation_course",y="math_score",data=df, color="g")
plt.subplot(1,3,2)
sns.boxplot(x="test_preparation_course",y="reading_score",data=df, color="r")
plt.subplot(1,3,3)
sns.boxplot(x="test_preparation_course",y="writing_score",data=df,color="y")

sns.heatmap(pd.crosstab(df["gender"],df["test_preparation_course"]),annot=True,fmt="d")

